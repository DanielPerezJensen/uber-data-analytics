{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb00b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "raw_data_path = os.path.join(data_path, \"bronze\")\n",
    "raw_data_file_path = os.path.join(raw_data_path, \"ncr_ride_bookings.csv\")\n",
    "processed_data_path = os.path.join(data_path, \"silver\")\n",
    "os.makedirs(raw_data_path, exist_ok=True)\n",
    "os.makedirs(processed_data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f872f344",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv(raw_data_file_path)\n",
    "\n",
    "\n",
    "def rename_columns(df):\n",
    "    \"\"\"Apply function to lowercase all columns and replace spaces with underscores\"\"\"\n",
    "    new_df = df.copy()\n",
    "\n",
    "    new_df.columns = new_df.columns.str.lower().str.replace(\" \", \"_\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = rename_columns(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1dd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_dtypes(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"date\"] = pd.to_datetime(new_df[\"date\"], errors=\"coerce\")\n",
    "    new_df[\"time\"] = pd.to_datetime(new_df[\"time\"], format=\"%H:%M:%S\", errors=\"coerce\").dt.time\n",
    "    # combine date and time columns to datetime\n",
    "    new_df[\"datetime\"] = pd.to_datetime(new_df[\"date\"].astype(str) + \" \" + new_df[\"time\"].astype(str), errors=\"coerce\")\n",
    "\n",
    "    new_df[\"booking_id\"] = new_df[\"booking_id\"].astype(pd.StringDtype())\n",
    "    new_df[\"booking_status\"] = new_df[\"booking_status\"].astype(\"category\")\n",
    "    new_df[\"customer_id\"] = new_df[\"customer_id\"].astype(pd.StringDtype())\n",
    "    new_df[\"vehicle_type\"] = new_df[\"vehicle_type\"].astype(\"category\")\n",
    "    new_df[\"pickup_location\"] = new_df[\"pickup_location\"].astype(pd.StringDtype())\n",
    "    new_df[\"drop_location\"] = new_df[\"drop_location\"].astype(\"category\")\n",
    "    new_df[\"avg_vtat\"] = pd.to_numeric(new_df[\"avg_vtat\"], errors=\"coerce\")\n",
    "    new_df[\"avg_ctat\"] = pd.to_numeric(new_df[\"avg_ctat\"], errors=\"coerce\")\n",
    "\n",
    "    # Add these columns and their types\n",
    "    new_df[\"reason_for_cancelling_by_customer\"] = new_df[\"reason_for_cancelling_by_customer\"].astype(\"category\")\n",
    "    new_df[\"driver_cancellation_reason\"] = new_df[\"driver_cancellation_reason\"].astype(\"category\")\n",
    "    new_df[\"cancelled_rides_by_driver\"] = pd.to_numeric(new_df[\"cancelled_rides_by_driver\"], errors=\"coerce\")\n",
    "    new_df[\"cancelled_rides_by_customer\"] = pd.to_numeric(new_df[\"cancelled_rides_by_customer\"], errors=\"coerce\")\n",
    "    new_df[\"incomplete_rides\"] = pd.to_numeric(new_df[\"incomplete_rides\"], errors=\"coerce\")\n",
    "    new_df[\"incomplete_rides_reason\"] = new_df[\"incomplete_rides_reason\"].astype(\"category\")\n",
    "    new_df[\"booking_value\"] = pd.to_numeric(new_df[\"booking_value\"], errors=\"coerce\")\n",
    "    new_df[\"ride_distance\"] = pd.to_numeric(new_df[\"ride_distance\"], errors=\"coerce\")\n",
    "    new_df[\"driver_ratings\"] = pd.to_numeric(new_df[\"driver_ratings\"], errors=\"coerce\")\n",
    "    new_df[\"customer_rating\"] = pd.to_numeric(new_df[\"customer_rating\"], errors=\"coerce\")\n",
    "    new_df[\"payment_method\"] = new_df[\"payment_method\"].astype(\"category\")\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930da78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df):\n",
    "    new_df = df.copy()\n",
    "    new_df[\"hour\"] = new_df[\"datetime\"].dt.hour\n",
    "    new_df[\"day\"] = new_df[\"datetime\"].dt.day\n",
    "    new_df[\"month\"] = new_df[\"datetime\"].dt.month\n",
    "    new_df[\"weekday\"] = new_df[\"datetime\"].dt.dayofweek  # Monday=0, Sunday=6\n",
    "    new_df[\"is_weekend\"] = new_df[\"weekday\"].isin([5, 6])\n",
    "\n",
    "    def segment_time_of_day(hour):\n",
    "        if 6 <= hour <= 11:\n",
    "            return \"morning\"\n",
    "        elif 12 <= hour <= 16:\n",
    "            return \"afternoon\"\n",
    "        elif 17 <= hour <= 22:\n",
    "            return \"evening\"\n",
    "        else:\n",
    "            return \"night\"\n",
    "\n",
    "    new_df[\"time_of_day\"] = new_df[\"hour\"].apply(lambda h: segment_time_of_day(h) if pd.notnull(h) else None)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "\n",
    "# Check result\n",
    "df[[\"datetime\", \"hour\", \"day\", \"month\", \"weekday\", \"is_weekend\", \"time_of_day\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f00ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"ðŸ“Œ Data Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nðŸ“Š Summary Statistics:\")\n",
    "print(df.describe(include=\"all\"))\n",
    "\n",
    "print(\"\\nðŸ” Null Values per Column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e986e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "\n",
    "\n",
    "# Create flags from cancellation and incomplete ride columns, null meaning it was NOT cancelled or incomplete\n",
    "def cancelled_to_flag(df):\n",
    "    new_df = df.copy()\n",
    "\n",
    "    columns_to_flag = [\n",
    "        \"cancelled_rides_by_driver\",\n",
    "        \"cancelled_rides_by_customer\",\n",
    "        \"incomplete_rides\",\n",
    "    ]\n",
    "\n",
    "    for col in columns_to_flag:\n",
    "        flag_col = f\"{col}_flag\"\n",
    "        new_df[flag_col] = new_df[col].notnull()\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def missing_to_flag(df):\n",
    "    new_df = df.copy()\n",
    "\n",
    "    columns_to_flag = [\n",
    "        \"driver_ratings\",\n",
    "        \"customer_rating\",\n",
    "        \"booking_value\",\n",
    "        \"payment_method\",\n",
    "    ]\n",
    "\n",
    "    for col in columns_to_flag:\n",
    "        flag_col = f\"{col}_missing_flag\"\n",
    "        new_df[flag_col] = new_df[col].isnull()\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = cancelled_to_flag(df)\n",
    "df = missing_to_flag(df)\n",
    "\n",
    "print(\n",
    "    df[\n",
    "        [\n",
    "            \"cancelled_rides_by_driver_flag\",\n",
    "            \"cancelled_rides_by_customer_flag\",\n",
    "            \"incomplete_rides_flag\",\n",
    "            \"driver_ratings_missing_flag\",\n",
    "            \"customer_rating_missing_flag\",\n",
    "            \"booking_value_missing_flag\",\n",
    "            \"payment_method_missing_flag\",\n",
    "        ]\n",
    "    ].sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e37465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73efb112",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "## Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a90b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# EDA\n",
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "df = cancelled_to_flag(df)\n",
    "df = missing_to_flag(df)\n",
    "\n",
    "df[\"customer_cancelled_target\"] = df[\"cancelled_rides_by_customer_flag\"].astype(int)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"pastel\")\n",
    "\n",
    "# 1. Cancellation Rate by Vehicle Type\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"vehicle_type\", y=\"customer_cancelled_target\", hue=\"vehicle_type\")\n",
    "plt.title(\"Customer Cancellation Rate by Vehicle Type\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2. Cancellation Rate by Hour\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"hour\", y=\"customer_cancelled_target\", hue=\"hour\")\n",
    "plt.title(\"Customer Cancellation Rate by Hour of Day\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Cancellation Rate by Day of Week\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(data=df, x=\"weekday\", y=\"customer_cancelled_target\", hue=\"weekday\")\n",
    "plt.title(\"Customer Cancellation Rate by Day of Week\")\n",
    "plt.xticks(ticks=range(7), labels=[\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"])\n",
    "plt.show()\n",
    "\n",
    "# 4. Cancellation Rate by Time of Day\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(\n",
    "    data=df,\n",
    "    x=\"time_of_day\",\n",
    "    y=\"customer_cancelled_target\",\n",
    "    order=[\"morning\", \"afternoon\", \"evening\", \"night\"],\n",
    "    hue=\"time_of_day\",\n",
    ")\n",
    "plt.title(\"Customer Cancellation Rate by Time of Day\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 5. Trend of Cancellations Over Time (Daily)\n",
    "df_daily = df.groupby(\"date\")[\"customer_cancelled_target\"].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.lineplot(data=df_daily, x=\"date\", y=\"customer_cancelled_target\")\n",
    "plt.title(\"Daily Customer Cancellation Rate Over Time\")\n",
    "plt.ylabel(\"Cancellation Rate\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Optional: Heatmap of Hour vs Day of Week\n",
    "heatmap_data = df.pivot_table(index=\"hour\", columns=\"weekday\", values=\"customer_cancelled_target\", aggfunc=\"mean\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\n",
    "plt.title(\"Cancellation Rate by Hour and Day of Week\")\n",
    "plt.ylabel(\"Hour of Day\")\n",
    "plt.xlabel(\"Day of Week (0=Mon, 6=Sun)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3e03b",
   "metadata": {},
   "source": [
    "## Location Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6a8cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# EDA\n",
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "df = cancelled_to_flag(df)\n",
    "df = missing_to_flag(df)\n",
    "\n",
    "df[\"customer_cancelled_target\"] = df[\"cancelled_rides_by_customer_flag\"].astype(int)\n",
    "\n",
    "sns.set_theme(style=\"darkgrid\", palette=\"pastel\")\n",
    "\n",
    "# 1. Cancellation rate by top 10 pickup locations ordered from high to low\n",
    "cancellation_rates = df.groupby(\"pickup_location\", observed=True)[\"customer_cancelled_target\"].mean()\n",
    "top_50_locations = cancellation_rates.sort_values(ascending=False).head(50)\n",
    "cumulative_sum = top_50_locations.cumsum()\n",
    "\n",
    "df_plot = top_50_locations.reset_index(name=\"cancellation_rate\")\n",
    "df_plot[\"cumulative_cancellation_rate\"] = cumulative_sum.values\n",
    "# Create a figure and a primary axis\n",
    "fig, ax1 = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# Plot the bar chart on the primary axis\n",
    "sns.barplot(\n",
    "    x=\"pickup_location\",\n",
    "    y=\"cancellation_rate\",\n",
    "    data=df_plot,\n",
    "    ax=ax1,\n",
    ")\n",
    "\n",
    "# Set the primary axis labels and title\n",
    "ax1.set_xlabel(\"Pickup Location\", fontsize=12)\n",
    "ax1.set_ylabel(\"Cancellation Rate\", fontsize=12)\n",
    "ax1.set_title(\"Top 50 Pickup Locations by Cancellation Rate and Cumulative Sum\", fontsize=16)\n",
    "ax1.tick_params(axis=\"x\", rotation=90)\n",
    "\n",
    "# Create a secondary axis for the line plot\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the cumulative sum line on the secondary axis\n",
    "sns.lineplot(\n",
    "    x=\"pickup_location\",\n",
    "    y=\"cumulative_cancellation_rate\",\n",
    "    data=df_plot,\n",
    "    ax=ax2,\n",
    "    color=\"red\",\n",
    "    marker=\"o\",\n",
    "    label=\"Cumulative Sum\",\n",
    ")\n",
    "\n",
    "# Set the secondary axis label\n",
    "ax2.set_ylabel(\"Cumulative Sum of Cancellation Rate\", fontsize=12)\n",
    "ax2.legend(loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968512e",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31973817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA\n",
    "df = rename_columns(df_raw)\n",
    "df = cast_to_dtypes(df)\n",
    "df = extract_temporal_features(df)\n",
    "df = cancelled_to_flag(df)\n",
    "feature_df = missing_to_flag(df)\n",
    "\n",
    "\n",
    "def calculate_rolling_counts(df, window, column_name):\n",
    "    new_df = df = df.sort_values(by=[\"customer_id\", \"datetime\"]).reset_index(drop=True)\n",
    "\n",
    "    # Perform the rolling calculation on a temporary DataFrame with a datetime index\n",
    "    temp_df = new_df.set_index(\"datetime\")\n",
    "    rolling_counts = temp_df.groupby(\"customer_id\")[\"booking_id\"].rolling(window=window).count()\n",
    "\n",
    "    # The result has a MultiIndex (customer_id, datetime). Reset it to columns.\n",
    "    rolling_counts = rolling_counts.reset_index(name=column_name)\n",
    "\n",
    "    # We now have a temporary DataFrame with customer_id, datetime, and the count.\n",
    "    # We need to shift the count to represent \"previous\" bookings.\n",
    "    rolling_counts[column_name] = rolling_counts.groupby(\"customer_id\")[column_name].shift(1).fillna(0).astype(int)\n",
    "\n",
    "    return rolling_counts\n",
    "\n",
    "\n",
    "feature_df[\"customer_cancelled_target\"] = feature_df[\"cancelled_rides_by_customer_flag\"].astype(int)\n",
    "\n",
    "# 1. Vehicle Type Encoding\n",
    "feature_df = pd.get_dummies(feature_df, columns=[\"vehicle_type\"], prefix=\"vehicle\", drop_first=True)\n",
    "\n",
    "# 2. weekday encoding\n",
    "feature_df = pd.get_dummies(feature_df, columns=[\"weekday\"], prefix=\"weekday\", drop_first=True)\n",
    "\n",
    "# 3. time_of_day encoding\n",
    "feature_df = pd.get_dummies(feature_df, columns=[\"time_of_day\"], prefix=\"tod\", drop_first=True)\n",
    "\n",
    "# 4. Bookings prev week, prev_month, prev_year\n",
    "# Calculate the rolling counts for each window\n",
    "bookings_prev_week = calculate_rolling_counts(feature_df, \"7D\", \"bookings_prev_week\")\n",
    "bookings_prev_month = calculate_rolling_counts(feature_df, \"30D\", \"bookings_prev_month\")\n",
    "bookings_prev_year = calculate_rolling_counts(feature_df, \"365D\", \"bookings_prev_year\")\n",
    "\n",
    "# Merge the calculated columns back into the original DataFrame\n",
    "feature_df = pd.merge(feature_df, bookings_prev_week, on=[\"customer_id\", \"datetime\"], how=\"left\")\n",
    "feature_df = pd.merge(feature_df, bookings_prev_month, on=[\"customer_id\", \"datetime\"], how=\"left\")\n",
    "feature_df = pd.merge(feature_df, bookings_prev_year, on=[\"customer_id\", \"datetime\"], how=\"left\")\n",
    "\n",
    "dropped_cols = [\n",
    "    \"booking_id\",\n",
    "    \"customer_id\",\n",
    "    \"pickup_location\",\n",
    "    \"drop_location\",\n",
    "    \"cancelled_rides_by_customer\",\n",
    "    \"reason_for_cancelling_by_customer\",\n",
    "    \"cancelled_rides_by_driver\",\n",
    "    \"driver_cancellation_reason\",\n",
    "    \"incomplete_rides\",\n",
    "    \"incomplete_rides_reason\",\n",
    "    \"datetime\",\n",
    "    \"hour\",\n",
    "    \"day\",\n",
    "    \"month\",\n",
    "    \"cancelled_rides_by_driver_flag\",\n",
    "    \"cancelled_rides_by_customer_flag\",\n",
    "    \"incomplete_rides_flag\",\n",
    "    \"driver_ratings_missing_flag\",\n",
    "    \"customer_rating_missing_flag\",\n",
    "    \"booking_value_missing_flag\",\n",
    "    \"payment_method_missing_flag\",\n",
    "]\n",
    "\n",
    "feature_df = rename_columns(feature_df)\n",
    "feature_df = feature_df.drop(columns=dropped_cols)\n",
    "target = \"customer_cancelled_target\"\n",
    "features = [col for col in feature_df if col != target]\n",
    "\n",
    "print(f\"Total Features for Modelling: {len(features)}\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ded781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights using sklearn\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0, 1]), y=feature_df[target])\n",
    "\n",
    "weights = dict(enumerate(class_weights))\n",
    "print(\"Class Weights:\", weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e343e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine X and y temporarily\n",
    "temp_df = feature_df.copy()\n",
    "\n",
    "boolean_cols = temp_df.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "for col in boolean_cols:\n",
    "    temp_df[col] = temp_df[col].astype(int)\n",
    "\n",
    "numeric_cols = temp_df.select_dtypes(include=[\"number\", \"bool\"]).columns\n",
    "\n",
    "# Compute correlation with target\n",
    "correlations = temp_df[numeric_cols].corr()[target].sort_values(key=abs, ascending=False)\n",
    "\n",
    "# Display top correlations (excluding target itself)\n",
    "print(\"ðŸ“Œ Top Correlated Features with Target:\\n\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05e1953",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "## Baseline - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_df_copy = feature_df.copy()\n",
    "\n",
    "boolean_cols = feature_df_copy.select_dtypes(include=\"bool\").columns\n",
    "\n",
    "for col in boolean_cols:\n",
    "    feature_df_copy[col] = feature_df_copy[col].astype(int)\n",
    "\n",
    "# Drop target and keep only numeric features\n",
    "clean_features = feature_df_copy.select_dtypes(include=[\"number\"]).columns.drop(target)\n",
    "\n",
    "# Setup X and y\n",
    "X = feature_df_copy[clean_features]\n",
    "y = feature_df_copy[target]\n",
    "\n",
    "# Impute missing values only for numeric columns\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Scaling\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(\"ðŸš‚ Features we are training on!\")\n",
    "print(X.columns)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train Logistic Regression\n",
    "logreg = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_prob = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "print(\"ðŸŽ¯ ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b374e9",
   "metadata": {},
   "source": [
    "## Model - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81abd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_df_copy = feature_df.copy()\n",
    "feature_df_copy = feature_df_copy.drop(columns=[\"date\", \"time\"])\n",
    "\n",
    "\n",
    "features = feature_df_copy.columns\n",
    "leaky_features = [\n",
    "    \"avg_vtat\",\n",
    "    \"ride_distance\",\n",
    "    \"driver_ratings\",\n",
    "    \"customer_ratings\",\n",
    "    \"booking_status\",\n",
    "    \"payment_method\",\n",
    "    target,\n",
    "]\n",
    "clean_features = [f for f in features if f not in leaky_features]\n",
    "\n",
    "# Prepare data\n",
    "X = feature_df_copy[clean_features]\n",
    "y = feature_df_copy[target]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "X_imputed = X.copy()\n",
    "X_imputed[numeric_cols] = imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train Random Forest with balanced class weights\n",
    "rf = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\", random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "y_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"ðŸŽ¯ ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc477f81",
   "metadata": {},
   "source": [
    "## Model - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdeb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_df_copy = feature_df.copy()\n",
    "feature_df_copy = feature_df_copy.drop(columns=[\"date\", \"time\"])\n",
    "\n",
    "\n",
    "features = feature_df_copy.columns\n",
    "leaky_features = [\n",
    "    \"avg_vtat\",\n",
    "    \"ride_distance\",\n",
    "    \"driver_ratings\",\n",
    "    \"customer_ratings\",\n",
    "    \"booking_status\",\n",
    "    \"payment_method\",\n",
    "    target,\n",
    "]\n",
    "clean_features = [f for f in features if f not in leaky_features]\n",
    "\n",
    "# Prepare data\n",
    "X = feature_df_copy[clean_features]\n",
    "y = feature_df_copy[target]\n",
    "\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[\"number\"]).columns\n",
    "X_imputed = X.copy()\n",
    "X_imputed[numeric_cols] = imputer.fit_transform(X[numeric_cols])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_imputed, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=13.29,\n",
    ")\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & Evaluate\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"ðŸŽ¯ ROC AUC Score:\", roc_auc_score(y_test, y_prob))\n",
    "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred, digits=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
